import sys
import json
import os

# --- IMPORTS FOR Hugging Face Transformers ---
# You'll need to install these libraries in your Python environment where this script will run:
# pip install transformers torch accelerate bitsandbytes sentencepiece
# Ensure torch is installed with CUDA support if you have an NVIDIA GPU:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Example for CUDA 11.8
from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline, BitsAndBytesConfig
import torch
# ---------------------------------------------

# --- MODEL LOADING ---
# Define the model ID for Mistral 7B Instruct v0.2
model_id = "mistralai/Mistral-7B-Instruct-v0.2"

# Global variable to hold the text generation pipeline
text_generator = None

def load_model():
    """Loads the LLM model and tokenizer into a text generation pipeline."""
    global text_generator
    if text_generator is not None:
        print("Debug: Model already loaded.", file=sys.stderr)
        return

    try:
        print(f"Debug: Attempting to load model: {model_id}...", file=sys.stderr)
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        # Set pad_token_id to eos_token_id to avoid issues with batching (even batch size 1 can benefit)
        if tokenizer.pad_token_id is None:
             tokenizer.pad_token_id = tokenizer.eos_token_id

        # Configure 4-bit quantization for lower VRAM usage
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for computation if possible
        )

        # Load the model with quantization and automatic device mapping
        # device_map="auto" attempts to put model layers on available GPUs and then CPU
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.bfloat16 # Use bfloat16 if supported for potentially better performance/VRAM
        )
        print("Debug: Model loaded with 4-bit quantization and device mapping.", file=sys.stderr)

        # Determine device for the pipeline
        device = 0 if torch.cuda.is_available() else -1
        if torch.cuda.is_available():
            print(f"Debug: CUDA available, using device {device}.", file=sys.stderr)
        else:
             print("Debug: CUDA not available, using CPU. Inference will be very slow.", file=sys.stderr)


        # Initialize the text generation pipeline
        text_generator = TextGenerationPipeline(
            model=model,
            tokenizer=tokenizer,
            device=device,
            # Set return_full_text=False to only get the generated part, not the prompt
            return_full_text=False
        )
        print("Debug: Text generation pipeline created successfully.", file=sys.stderr)

    except Exception as e:
        print(f"Error loading model: {e}", file=sys.stderr)
        # Set generator to None to indicate failure
        text_generator = None

def customize_resume_with_llm(job_post: str, resume: str) -> str:
    """
    Uses the loaded Mistral 7B Instruct v0.2 model with prompt engineering to customize a resume.

    Args:
        job_post: The text content of the job description.
        resume: The text content of the user's resume.

    Returns:
        The customized resume text generated by the LLM, or an error message.
    """
    # Ensure the model is loaded before attempting to use it
    if text_generator is None:
         print("Debug: Model not loaded, attempting to load now.", file=sys.stderr)
         load_model()
         if text_generator is None:
            return "Error: LLM model failed to load. Cannot customize resume."

    print("Debug: Received job_post and resume data for customization.", file=sys.stderr)

    # --- PROMPT ENGINEERING: Craft a detailed prompt for the LLM ---
    # Using the instruct format [INST] ... [/INST] for Mistral Instruct models.
    prompt = f"""[INST] You are an expert career coach and resume writer. Your task is to take a user's resume and tailor it specifically for a given job description.

Follow these steps carefully:
1.  **Analyze the Job Description:** Read the provided job description thoroughly. Identify the key requirements, required skills (both technical and soft), qualifications, responsibilities, company culture hints, and overall tone. Prioritize the most important keywords and phrases used in the job post.
2.  **Analyze the User's Resume:** Read the user's resume. Identify their skills, experience, projects, education, and profile summary.
3.  **Match and Customize:** Compare the job description requirements with the user's resume content.
    *   **Objective or Summary Section:** Rewrite to clearly state how your goals and skills align with the target job role. Emphasize keywords and soft/hard skills relevant to the job.
    *   **Skills Section:** Highlight the skills that match the job requirements. Remove or de-prioritize unrelated skills. Add keywords used in the job description (e.g., specific programming languages, tools, methodologies).
    *   **Work Experience / Projects:** Focus on relevant experience and projects that demonstrate your ability to do the specific job. Reword bullet points to reflect the language of the job description. Quantify achievements that relate to the job role.
    *   **Education and Certifications:** Emphasize relevant degrees, certifications, or courses that the job demands. De-emphasize unrelated education.
    *   **Additional Sections (optional):** Add or remove sections like "Awards," "Volunteer Experience," or "Publications" depending on relevance to the job post.
4.  **Maintain Professional Formatting:** Present the customized resume clearly. Use standard resume sections (Contact Information, Summary/Objective, Skills, Experience/Projects, Education, Certifications, etc.). Use clear headings and bullet points (use - or *). Maintain a professional and easy-to-read layout using plain text formatting. Ensure contact information is at the top. Do NOT include markdown formatting that renders visually (like bolding with **, or code blocks with ```).
5.  **Tone:** Adopt a confident, professional, and results-oriented tone that matches the likely tone of the job description.
6.  **Output:** Provide ONLY the complete customized resume text in plain text format. Do not include any conversational filler, introductions, or explanations before or after the resume text.

Here is the Job Description:
---
{job_post}
---

Here is the User's Original Resume:
---
{resume}
---

Please provide the customized resume based on these instructions: [/INST]
"""
    # Using the instruct format [INST] ... [/INST] for Mistral Instruct models.
    # -----------------------------------------------------------

    # --- LLM Inference using the pipeline ---
    try:
        print("Debug: Generating text with LLM pipeline...", file=sys.stderr)

        # Adjust generation parameters as needed
        response = text_generator(
            prompt,
            max_new_tokens=1500, # Maximum number of tokens to generate
            num_return_sequences=1, # We only need one generated resume
            do_sample=True, # Use sampling for more varied output
            top_k=50,       # Consider top 50 most likely tokens
            top_p=0.95,     # Use nucleus sampling
            temperature=0.7,# Control creativity (lower = more focused)
            # The pipeline with return_full_text=False should handle removing the prompt
        )[0]

        customized_text = response['generated_text']

        print("Debug: Finished text generation.", file=sys.stderr)

        # Although return_full_text=False is used, sometimes partial prompt
        # or leading/trailing whitespace might remain depending on the library version
        # and model. Basic cleaning can be added here if necessary after testing.
        # For Mistral Instruct, the output usually starts right after [/INST].

        return customized_text.strip() # Strip leading/trailing whitespace

    except Exception as e:
        print(f"Error during LLM inference: {e}", file=sys.stderr)
        # Return an informative error message if the LLM call fails
        return "Error: Could not generate customized resume using the LLM. Please check the backend logs for inference errors."
    # ------------------------------------


if __name__ == "__main__":
    # Load the model once when the script starts (if it's not already loaded)
    # This helps subsequent calls be faster.
    load_model()

    try:
        # Read the JSON input from stdin
        # The Node.js backend sends { jobPost: "...", resume: "..." } as JSON to stdin.
        input_data = json.load(sys.stdin)
        job_post = input_data.get('jobPost', '')
        resume = input_data.get('resume', '')

        if not job_post or not resume:
            print("Error: Missing jobPost or resume in input.", file=sys.stderr)
            # Print an error message that the frontend can display
            print("No customized resume generated: Missing job post or resume input.")
            sys.exit(1) # Exit with a non-zero code to indicate an error

        # Call the function that uses the LLM
        customized_resume_text = customize_resume_with_llm(job_post, resume)

        # Print the result to stdout
        # The Node.js backend will capture this output.
        # Ensure the output is just the text resume.
        print(customized_resume_text)

        sys.exit(0) # Exit successfully

    except json.JSONDecodeError:
        print("Error: Invalid JSON input received by script.", file=sys.stderr)
        print("No customized resume generated: Invalid input data.")
        sys.exit(1) # Exit with an error code
    except Exception as e:
        # Catch any other unexpected errors during script execution
        print(f"An unhandled error occurred in the Python script: {e}", file=sys.stderr)
        print("Error customizing resume. An internal script error occurred.")
        sys.exit(1) # Exit with an error code 